{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Midterm Assignment: McDonald's Sentiment Data Analysis - Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem\n",
    "\n",
    "McDonald’s receives thousands of consumer comment on their website every day and many of them are negative. Their corporate employees do not have the time to browse through every single comment, but they do want to read a subset that they are most interested in. In particular, articles about the rude service of their employees have recently surfaced on social media. In order to take appropriate action, they would now like to review comments about **rude service**. \n",
    "\n",
    "You are hired to develop a system that ranks each comment by the **likelihood that it is referring to rude service**. They will use this system to build a “rudeness dashboard” for their corporate employees, so that the employees can spend a few minutes each day examining the **most relevant recent comments**.\n",
    "\n",
    "\n",
    "## Data\n",
    "\n",
    "McDonald’s used the CrowdFlower platform to pay humans to hand-annotate approximately 1500 comments with the type of complaint. The list of complaint types can be found below, with the encoding used listed in parentheses: \n",
    "- Bad Food (BadFood)\n",
    "- Bad Neighborhood (ScaryMcDs)\n",
    "- Cost (Cost)\n",
    "- Dirty Location (Filthy)\n",
    "- Missing Item (MissingFood)\n",
    "- Problem with Order (OrderProblem)\n",
    "- Rude Service (RudeService)\n",
    "- Slow Service (SlowService)\n",
    "- None of the above (na) \n",
    "\n",
    "You will be asked to perform some tasks. In the midst of these tasks, some MCQs will be asked. You are to select the best possible option as your answer. Please answer them accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for Python 2: use print only as a function\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "\n",
    "Read **'mcdonalds.csv'** into a pandas DataFrame and examine it. (Instructions: mcdonalds.csv can be found in “IVLE Workbin > Midterm Assignment > data”) \n",
    "\n",
    "A description of the more important columns to get you started: \n",
    "- The **policies_violated** column lists the type of complaint. If there is more than one type, the types are separated by newline characters.\n",
    "- The **policies_violated:confidence** column lists CrowdFlower's confidence in the judgments of its human annotators for that row (higher is better).\n",
    "- The **city** column is the McDonald's location.\n",
    "- The **review** column is the actual text comment.\n",
    "\n",
    "**Please answer Question 1 as in midterm.pdf.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read mcdonalds.csv using a relative path\n",
    "import pandas as pd\n",
    "path = 'mcdonalds.csv'\n",
    "mcd = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>_golden</th>\n",
       "      <th>_unit_state</th>\n",
       "      <th>_trusted_judgments</th>\n",
       "      <th>_last_judgment_at</th>\n",
       "      <th>policies_violated</th>\n",
       "      <th>policies_violated:confidence</th>\n",
       "      <th>city</th>\n",
       "      <th>policies_violated_gold</th>\n",
       "      <th>review</th>\n",
       "      <th>Unnamed: 10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>679455653</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>2/21/15 0:36</td>\n",
       "      <td>RudeService\\nOrderProblem\\nFilthy</td>\n",
       "      <td>1.0\\n0.6667\\n0.6667</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm not a huge mcds lover, but I've been to be...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>679455654</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>2/21/15 0:27</td>\n",
       "      <td>RudeService</td>\n",
       "      <td>1</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Terrible customer service. ŒæI came in at 9:30...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>679455655</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>2/21/15 0:26</td>\n",
       "      <td>SlowService\\nOrderProblem</td>\n",
       "      <td>1.0\\n1.0</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>NaN</td>\n",
       "      <td>First they \"lost\" my order, actually they gave...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    _unit_id _golden _unit_state  _trusted_judgments _last_judgment_at  \\\n",
       "0  679455653   False   finalized                   3      2/21/15 0:36   \n",
       "1  679455654   False   finalized                   3      2/21/15 0:27   \n",
       "2  679455655   False   finalized                   3      2/21/15 0:26   \n",
       "\n",
       "                   policies_violated policies_violated:confidence     city  \\\n",
       "0  RudeService\\nOrderProblem\\nFilthy          1.0\\n0.6667\\n0.6667  Atlanta   \n",
       "1                        RudeService                            1  Atlanta   \n",
       "2          SlowService\\nOrderProblem                     1.0\\n1.0  Atlanta   \n",
       "\n",
       "   policies_violated_gold                                             review  \\\n",
       "0                     NaN  I'm not a huge mcds lover, but I've been to be...   \n",
       "1                     NaN  Terrible customer service. ŒæI came in at 9:30...   \n",
       "2                     NaN  First they \"lost\" my order, actually they gave...   \n",
       "\n",
       "   Unnamed: 10  \n",
       "0          NaN  \n",
       "1          NaN  \n",
       "2          NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the first three rows\n",
    "mcd.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Ok I'm waiting for like 10 minutes to place my order with the staff walking back & forth just looking at me like I'm crazy. And another 10 minutes or so before i got my food, This location use to be my stop in the mornings when I worked near here but they have fallen way off.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the text of the 128th review\n",
    "mcd.loc[127, 'review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "\n",
    "Remove any rows from the DataFrame in which the policies_violated column has a null value.\n",
    "- **Note**: Null values are also known as “missing values”, and are encoded in pandas with the special value “NaN’. This is different from the “na” encoding used by CrowdFlower to denote “None of the above”. Rows that contain “na” should not be removed. \n",
    "\n",
    "**Please answer Questions 2 and 3 as in midterm.pdf.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1525, 11)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the shape before removing any rows\n",
    "mcd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_unit_id                           0\n",
       "_golden                            0\n",
       "_unit_state                        0\n",
       "_trusted_judgments                 0\n",
       "_last_judgment_at                  0\n",
       "policies_violated                 54\n",
       "policies_violated:confidence      54\n",
       "city                              87\n",
       "policies_violated_gold          1525\n",
       "review                             0\n",
       "Unnamed: 10                     1525\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count the number of null values in each column\n",
    "mcd.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the DataFrame to only include rows in which policies_violated is not null\n",
    "mcd = mcd[mcd.policies_violated.notnull()]\n",
    "\n",
    "# alternatively, use the 'dropna' method to accomplish the same thing\n",
    "mcd.dropna(subset=['policies_violated'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1471, 11)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the shape after removing rows\n",
    "mcd.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "\n",
    "Add a new column to the DataFrame called **\"rude\"** that is 1 if the **policies_violated** column contains the text \"RudeService\", and 0 if the **policies_violated** column does not contain \"RudeService\". The \"rude\" column is going to be your response variable, so check how many zeros and ones it contains.\n",
    "\n",
    "**Please answer Question 4 as in midterm.pdf.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# search for the string 'RudeService', and convert the boolean results to integers\n",
    "mcd['rude'] = mcd.policies_violated.str.contains('RudeService').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policies_violated</th>\n",
       "      <th>rude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RudeService\\nOrderProblem\\nFilthy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RudeService</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SlowService\\nOrderProblem</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>na</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RudeService</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   policies_violated  rude\n",
       "0  RudeService\\nOrderProblem\\nFilthy     1\n",
       "1                        RudeService     1\n",
       "2          SlowService\\nOrderProblem     0\n",
       "3                                 na     0\n",
       "4                        RudeService     1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm that it worked\n",
    "mcd.loc[0:4, ['policies_violated', 'rude']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    968\n",
       "1    503\n",
       "Name: rude, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the class distribution\n",
    "mcd.rude.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.658056\n",
       "1    0.341944\n",
       "Name: rude, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcd.rude.value_counts()/sum(mcd.rude.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4\n",
    "\n",
    "Define X using the **review** column and y using the **rude** column. Split X and y into training and testing sets (using the parameter **`random_state=1`**). Use CountVectorizer (with the **default parameters**) to create document-term matrices from X_train and X_test. \n",
    "- Note: Please remember to follow the instructions carefully by setting the parameters as required for reproducibility of results. \n",
    "\n",
    "**Please answer Questions 5 and 6 as in midterm.pdf.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define X and y\n",
    "X = mcd.review\n",
    "y = mcd.rude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jovianlin/anaconda3/envs/python3/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# split X and y into training and testing sets\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import and instantiate CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1103, 7300)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit and transform X_train into X_train_dtm\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_train_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(368, 7300)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform X_test into X_test_dtm\n",
    "X_test_dtm = vect.transform(X_test)\n",
    "X_test_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x7300 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 3 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_dtm[24]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5\n",
    "\n",
    "Fit a Multinomial Naive Bayes model to the training set, calculate the **predicted probabilities** for the testing set, and then calculate the AUC. Repeat this task using a logistic regression model to compare which of the two models achieves a better AUC. \n",
    "- **Note**: McDonald’s requires you to rank the comments by the likelihood that they refer to rude service. In this case, classification accuracy is NOT the relevant evaluation metric. Area Under Curve (AUC) is a more useful evaluation metric for this scenario, since it measures the ability of the classifier to assign higher predicted probabilities to positive instances than to negative instances. \n",
    "\n",
    "**Please answer Questions 7, 8 and 9 as in midterm.pdf.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import/instantiate/fit a Multinomial Naive Bayes model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_dtm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate the predicted probability of rude=1 for each testing set observation\n",
    "y_pred_prob = nb.predict_proba(X_test_dtm)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40000000000000002"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = nb.predict(X_test_dtm)\n",
    "sum(y_pred[0:5])/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84260054045461774"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the AUC\n",
    "from sklearn import metrics\n",
    "nb_AUC = metrics.roc_auc_score(y_test, y_pred_prob)\n",
    "nb_AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82339850580193941"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# repeat this task using a logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train_dtm, y_train)\n",
    "y_pred_prob = logreg.predict_proba(X_test_dtm)[:, 1]\n",
    "logreg_AUC = metrics.roc_auc_score(y_test, y_pred_prob)\n",
    "logreg_AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.019202034652678335"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_AUC - logreg_AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6\n",
    "\n",
    "Using Naive Bayes, try **tuning CountVectorizer** using some of the techniques we learned in class. Check the testing set AUC after each change, and find the set of parameters that increases AUC the most. (This is meant for your own learning experience)\n",
    "- **Hint**: It is highly recommended that you adapt the **`tokenize_test()`** function from class for this purpose, since it will allow you to iterate quickly through different sets of parameters. \n",
    "\n",
    "**Please answer Questions 10 and 11 as in midterm.pdf.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a function that accepts a vectorizer and calculates the AUC\n",
    "def tokenize_test(vect):\n",
    "    \n",
    "    # create document-term matrices using the vectorizer\n",
    "    X_train_dtm = vect.fit_transform(X_train)\n",
    "    X_test_dtm = vect.transform(X_test)\n",
    "    \n",
    "    # print the number of features that were generated\n",
    "    print('Features:', X_train_dtm.shape[1])\n",
    "    \n",
    "    # use Multinomial Naive Bayes to calculate predicted probabilities\n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(X_train_dtm, y_train)\n",
    "    y_pred_prob = nb.predict_proba(X_test_dtm)[:, 1]\n",
    "    \n",
    "    # print the AUC\n",
    "    print('AUC:', metrics.roc_auc_score(y_test, y_pred_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 7300\n",
      "AUC: 0.842600540455\n"
     ]
    }
   ],
   "source": [
    "# confirm that the AUC is identical to task 5 when using the default parameters\n",
    "vect = CountVectorizer()\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 1732\n",
      "AUC: 0.862152281036\n"
     ]
    }
   ],
   "source": [
    "# tune CountVectorizer to increase the AUC\n",
    "vect = CountVectorizer(stop_words='english', max_df=0.3, min_df=4)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7 \n",
    "\n",
    "The city column might be predictive of the response, but we are currently not using it as a feature. We will now explore to see if we can increase the AUC by adding city to the model. You are to do the following: \n",
    "1. Create a new DataFrame column, review_city, that concatenates the review text with the city text. One easy way to combine string columns in pandas is by using the `Series.str.cat()` method. Make sure to use the whitespace character as a separator, as well as replacing null city values with a reasonable string value such as ‘na’. \n",
    "2. Redefine X using the review_city column, and re-split X and y into training and testing sets (using the parameter `random_state=1`). \n",
    "3. By allowing for English stopwords removal, and setting the following parameters as `max_df = 0.3`, `min_df=4` in the CountVectorizer, check whether it has increased or decreased the AUC. \n",
    "\n",
    "**Please answer Question 12 as in midterm.pdf.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate review and city, separated by a space, replacing nulls with 'na'\n",
    "mcd['review_city'] = mcd.review.str.cat(mcd.city, sep=' ', na_rep='na')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm not a huge mcds lover, but I've been to better ones. This is by far the worst one I've ever been too! It's filthy inside and if you get drive through they completely screw up your order every time! The staff is terribly unfriendly and nobody seems to care. Atlanta\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine review_city for the first row to confirm that it worked\n",
    "mcd.loc[0, 'review_city']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# redefine X and y\n",
    "X = mcd.review_city\n",
    "y = mcd.rude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# re-split X and y into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 1739\n",
      "AUC: 0.864854554125\n"
     ]
    }
   ],
   "source": [
    "# check whether it increased or decreased the AUC of my best model\n",
    "vect = CountVectorizer(stop_words='english', max_df=0.3, min_df=4)\n",
    "tokenize_test(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0027022730879999735"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.864934032745 - 0.862231759657"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8 \n",
    "\n",
    "The **policies_violated:confidence** column may be useful as it is a measure of the training data quality. You are to calculate the **mean confidence** score for each row of your McDonald’s dataset (i.e. X_train together with X_test) and store these mean scores in a new column. For example the confidence scores for the first row are 1.0\\r\\n0.6667\\r\\n0.6667, so you should calculate a mean of 0.7778. Here are some of the steps you can follow: \n",
    "1. Using the `Series.str.split()` method, convert the policies_violated:confidence column into lists of one or more “confidence scores”. Save the results as a new DataFrame column called **confidence_list**. \n",
    "2. Apply a function that can calculate the mean of a list of numbers, and pass that function to the `Series.apply()` method of the **confidence_list** column. Save those scores in a new DataFrame column called **confidence_mean**. \n",
    "\n",
    "**Please answer Question 13 as in midterm.pdf.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1.0\\n0.6667\\n0.6667\n",
       "1                      1\n",
       "2               1.0\\n1.0\n",
       "3                 0.6667\n",
       "4                      1\n",
       "Name: policies_violated:confidence, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the existing confidence column\n",
    "mcd['policies_violated:confidence'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [1.0, 0.6667, 0.6667]\n",
       "1                      [1]\n",
       "2               [1.0, 1.0]\n",
       "3                 [0.6667]\n",
       "4                      [1]\n",
       "Name: confidence_list, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split the column into lists of one or more confidence scores\n",
    "mcd['confidence_list'] = mcd['policies_violated:confidence'].str.split()\n",
    "mcd.confidence_list.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# define a function that accepts a list of strings and returns the mean\n",
    "def mean_of_list(conf_list):\n",
    "    \n",
    "    # convert the list to a NumPy array of floats\n",
    "    conf_array = np.array(conf_list, dtype=float)\n",
    "    \n",
    "    # return the mean of the array\n",
    "    return np.mean(conf_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.7778\n",
       "1    1.0000\n",
       "2    1.0000\n",
       "3    0.6667\n",
       "4    1.0000\n",
       "Name: confidence_mean, dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the mean confidence score for each row\n",
    "mcd['confidence_mean'] = mcd.confidence_list.apply(mean_of_list)\n",
    "mcd.confidence_mean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "785"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(mcd.confidence_mean==1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now like to remove lower-quality rows from the training set to reduce noise. You are to remove all rows from X_train and y_train that have a confidence_mean lower than 0.75. \n",
    "\n",
    "**Please answer Questions 14 and 15 as in midterm.pdf.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1103,)\n",
      "(1103,)\n"
     ]
    }
   ],
   "source": [
    "# check the shapes of X_train and y_train before removing any rows\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(799,)\n",
      "(799,)\n"
     ]
    }
   ],
   "source": [
    "# remove any rows from X_train and y_train that have a confidence_mean lower than 0.75\n",
    "X_train = X_train[mcd.confidence_mean >= 0.75]\n",
    "y_train = y_train[mcd.confidence_mean >= 0.75]\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 1353\n",
      "AUC: 0.849690033381\n"
     ]
    }
   ],
   "source": [
    "# check whether it increased or decreased the AUC of my best model\n",
    "vect = CountVectorizer(stop_words='english', max_df=0.3, min_df=4)\n",
    "tokenize_test(vect)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
